# Initial import of libraries used in the code
import pandas as pd

# Reading the catalog schema and creating the loop and variable
databases = [
    db.databaseName 
    for db in spark.sql('show databases in your_catalog').collect()
]
tables = [
    f"{row['tableName']}"
    for db_rows in [
        spark.sql(f'show tables in your_catalog.yourschema').collect() for db in databases
    ] 
    for row in db_rows
]
print(tables)

def comments():
    for table in tables:

        catalog = 'your_catalog'
        schema = 'your_schema'
        table = table
        table_name = f"{catalog}.{schema}.{table}"
        
        # Creating the SQL query with the table name dynamically inserted
        query = f"""
        CREATE OR REPLACE FUNCTION GENERATE_COLUMN_DESCRIPTION(col_name STRING, data_type STRING)
        RETURNS array<struct<column_description:string, cd:date>>
        RETURN 
        SELECT FROM_JSON(
            ASK_LLM_MODEL(
            CONCAT(
            'Generate a business column description to be used as metadata for the column in table `{table_name}`, ',
            'based on the following column name: ', col_name, 
            ' and data type: ', data_type, 
            '. The descriptions should be complete, accurate to the data type, and based on the first 50 rows of data from the column for better accuracy. These descriptions will primarily assist the **Genie in Databricks** to make accurate queries and correlations between tables. Therefore, they need to have proper metadata and establish relationships with other tables.',
            'Give me JSON only. No text outside JSON. No explanations or notes. ',
            '[{{"column_description":<string>, "cd":<', current_date(),'">}}]'
            ) 
            ),
            "array<struct<column_description:string, cd:date>>"
        )
        """

        # Executing the SQL query in Databricks
        display(spark.sql(query))

        # Building the query with pre-filled values
        query = f"""
        CREATE OR REPLACE TEMPORARY VIEW annotated_table 
        COMMENT "temp view to store column descriptions" AS
        SELECT
        column_name,
        data_type,
        concat(
            temp_description.column_description, 
            " --- Definition generated by Databricks AI on ", 
            current_date()
        ) as column_description,
        temp_description.cd
        FROM
        (
            SELECT
            column_name, 
            data_type, 
            explode(column_description) as temp_description
            FROM
            (
                SELECT column_name, data_type,
                GENERATE_COLUMN_DESCRIPTION(column_name, data_type) as column_description
                FROM (
                SELECT ordinal_position, column_name, data_type
                FROM {catalog}.information_schema.columns
                WHERE table_schema = '{schema}'
                    AND table_name = '{table}'
                ORDER BY ordinal_position
                )
            ) as temp_descriptions
        )
        """

        # Executing the query SQL in Databricks
        display(spark.sql(query))

        # Executing the SQL query and storing the result in a PySpark DataFrame
        column_descriptions = spark.sql("""
        SELECT column_name, data_type, column_description, cd
        FROM annotated_table
        """)

        # Convert the Spark DataFrame to a Pandas DataFrame and then to a dictionary
        column_descriptions = column_descriptions.toPandas().to_dict(orient="records")

        # Apply generated column descriptions to the UC table using ALTER
        for col in column_descriptions:
            try:
                # Escape single quotes in comments
                escaped_comment = col['column_description'].replace(r"'", r"\'")
                spark.sql(f"""
                ALTER TABLE {table_name} 
                ALTER COLUMN {col['column_name']} 
                COMMENT '{escaped_comment}'
                """)
                print(f"Successfully added comment to column: {table_name}")
            except Exception as e:
                # If an error occurs, log it and continue with the next column
                print(f"Error adding comment to column {table_name}: {str(e)}")

comments()
